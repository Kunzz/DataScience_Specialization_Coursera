---
title: "Practical Machine Learning Project, Data Science Specialization, Coursera"
author: "Kun Zhu"
date: "January 23, 2016"
output: html_document
---
### Purpose 
The purpose of this project is to create a data model to predict the quality of a common weight lifting exercise, dumbbell biceps curl, performed by six test subjects. The data are collected from a set of wearable accelarometers. This is an emerging subject in the Human Activity Recognition (HAR) research area. Detailed project specification can be found: https://www.coursera.org/learn/practical-machine-learning/peer/R43St/prediction-assignment-writeup.

### Data
The data used in this study is downloaded from http://groupware.les.inf.puc-rio.br/har. 
There are in total 276 candidate predictor variables in the data set. The dependent variable is the classification index, i.e., class A to E, which is defined to benchmark the quality of the exercise. The prediction model will be used to classify each movement into a set of categories. This is a typical classification machine learning problem. 

### Data wrangling 
Download the training and testing data set and save as data frames.
A preliminary examination of the .csv files suggest that there are three types of missing values: "NA", "#DIV/0!" and " ". For consistency, the last two types are all converted to "NA".
```{r}
suppressMessages(library(caret))
suppressMessages(library(rpart))
suppressMessages(library(rpart.plot))
suppressMessages(library(rattle))
suppressMessages(library(randomForest))
set.seed(1118)
trainUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testUrl <- "http://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
training <- read.csv(url(trainUrl), na.strings=c("#DIV/0!",""))
testing <- read.csv(url(testUrl), na.strings=c("#DIV/0!",""))
```
Partitioning the `training` data set into `actTraining`, 80%, and `validation`, 20%.
```{r}
dataPart <- createDataPartition(y=training$classe, p=0.2, list=FALSE)
validation <- training[dataPart, ]
actTraining <- training[-dataPart, ]
dim(actTraining)
dim(validation)
```
Removing the predictor variables that are obviously irrelevant to the dependent variable, classification index. Specially, the data entry index in column 1, time stamp in column 3, 4 and 5, `new_window` in column 6, and `num_window` in column 7 are excluded from the data set. It is important to note that the `num_window` and `new_window` are the sliding windows when the measurements are taken. By intuition, this is a less relevant factor to the performance of the exercise, see http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf for details. 
```{r}
actTraining <- actTraining[c(-1, -3, -4, -5, -6, -7)]
```
Only predict variables without any "NA"s are considered in this study. The the columns indexed in `incompleteData` are removed from the data set.
```{r}
incompleteData <- sapply(actTraining, function (x) any(is.na(x) | x == "NA"))
actTraining <- actTraining[!incompleteData]
dim(actTraining)
```
Next, `nearZeroVar` function from the `caret` package is used to identify predictors with zero or near zero variance. These predictors are again excluded from the data set. The study below suggests that none of predictor variables return zero or near-zero variance.
Therefore, the number of columns remain the same as in the previous step.
```{r}
NZV <- nearZeroVar(actTraining, saveMetrics=TRUE)
actTraining <- actTraining[!(NZV$zeroVar + NZV$nzv)]
dim(actTraining)
```
Recording the columns names of the data set after data wrangling together with the last variable `classe`, which is the dependent variable to be predicted. Repeat the data wrangling procedure to the validation and testing data set. 
```{r}
wrangle <- colnames(actTraining)
validation <- validation[wrangle]
# testing data has no "classe" column, colnum 54.
testing <- testing[wrangle[1:53]] 
dim(validation)
dim(testing)
```
### Predictions
As mentioned above, the purpose of this study is to classify examples given a set of categories. This is a typical classification machine learning problem. Two of the most common methods for the classification problem, Decision tree and Random forest, both considered in this study. Their in-sample prediction accuracy are summarized below.

#### Decision Tree

```{r}
modFitDT <- rpart(classe ~ ., data=actTraining, method="class")
fancyRpartPlot(modFitDT)
predictionDT <- predict(modFitDT, validation, type = "class")
confusionMatrix(predictionDT, validation$classe)
```

#### Random Forest
```{r}
modFitRF <- randomForest(classe ~. , data=actTraining)
prediction2 <- predict(modFitRF, validation, type = "class")
confusionMatrix(prediction2, validation$classe)

```
It is clearly that Random Forest method outperforms the Decision Tree by providing more accurate estimation, given the validation data set. Therefore, the prediction model, `modFitRF` using the Random Forest are selected and applied to the `testing` data set.
```{r}
predictionFinal <- predict(modFitRF, testing, type = "class")
```
The predicted class index for the `testing` data set are:
```{r}
predictionFinal
```